{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "#  Keras provides the text to word sequence() function that you can use to split text into a list of words.\n",
    "# by default this fn does 3 things 1) split by spaces 2) remove punc 3) loweccase\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "\n",
    "result = text_to_word_sequence(text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[7, 3, 4, 6, 8, 6, 7, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "# Encoding with one_hot\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "\n",
    "result = one_hot(text,round(vocab_size*1.3))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "# Hash encoding with hashing trick\n",
    "\n",
    "#A limitation of integer and count base encodings is that they must maintain a vocabulary of words and their mapping to integers. An alternative to this approach is to use a one-way hash\n",
    "#function to convert words to integers. This avoids the need to keep track of a vocabulary, which\n",
    "#is faster and requires less memory\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import hashing_trick\n",
    "\n",
    "result = hashing_trick(text,round(vocab_size*1.3),hash_function='md5')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have looked at one-off convenience methods for preparing text with Keras. Keras\n",
    "provides a more sophisticated API for preparing text that can be fit and reused to prepare\n",
    "multiple text documents. This may be the preferred approach for large projects. Keras provides\n",
    "the Tokenizer class for preparing text documents for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# define 5 documents\n",
    "docs = ['Well done!',\n",
    "'Good work',\n",
    "'Great effort',\n",
    "'nice work',\n",
    "'Excellent!']\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "t.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fit, the Tokenizer provides 4 attributes that you can use to query what has been\n",
    "learned about your documents:\n",
    " word counts: A dictionary of words and their counts.\n",
    " word docs: An integer count of the total number of documents that were used to fit the\n",
    "Tokenizer.\n",
    " word index: A dictionary of words and their uniquely assigned integers.\n",
    " document count: A dictionary of words and how many documents each appeared in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'work': 2, 'good': 1, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n"
     ]
    }
   ],
   "source": [
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Tokenizer has been fit on training data, it can be used to encode documents in\n",
    "the train or test datasets. The texts to matrix() function on the Tokenizer can be used to\n",
    "create one vector per document provided per input. The length of the vectors is the total size\n",
    "of the vocabulary. This function provides a suite of standard bag-of-words model text encoding\n",
    "schemes that can be provided via a mode argument to the function. The modes available\n",
    "include:\n",
    " binary: Whether or not each word is present in the document. This is the default.\n",
    " count: The count of each word in the document.\n",
    " tfidf: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word\n",
    "in the document.\n",
    " freq: The frequency of each word as a ratio of words within each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "encoded_docs = t.texts_to_matrix(docs,mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
